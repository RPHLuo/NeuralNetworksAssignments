{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Analysis_ConvandLSTM.ipynb","version":"0.3.2","provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-toeHIMwmhhN","colab_type":"text"},"cell_type":"markdown","source":["## Import libraries"]},{"metadata":{"scrolled":true,"id":"YxOFkD-BmhhQ","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P3D3mwv4mhhc","colab_type":"text"},"cell_type":"markdown","source":["## Setup some configurational parameters"]},{"metadata":{"id":"d9YaGTFimhhd","colab_type":"code","colab":{}},"cell_type":"code","source":["MAX_NB_WORDS=200000\n","MAX_SEQUENCE_LENGTH=50\n","VALIDATION_SPLIT = .2\n","GLOVE_DIR=\"../glove.6B\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"uXrnQvlVmhhk","colab_type":"code","colab":{}},"cell_type":"code","source":["with open(\"/content/gdrive/My Drive/NN/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\" ) as f:\n","    li=f.readlines()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RTQnph8Emhhn","colab_type":"code","colab":{}},"cell_type":"code","source":["li[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lok3I_ojmhhw","colab_type":"code","colab":{}},"cell_type":"code","source":["texts = []  # list of text samples\n","labels = []  # list of label ids\n","\n","\n","for row in li: \n","    row = row.replace('\"',\"\").strip().split(\",\")\n","    texts.append(row[-1])\n","    if int(row[0])==4:\n","        label = 1\n","    elif int(row[0]) == 2:\n","        label = 1\n","    else:\n","        label =0\n","    labels.append(label)\n","\n","print('Found %s texts.' % len(texts))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZUwnv_vdmhh-","colab_type":"code","colab":{}},"cell_type":"code","source":["labels.count(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rgTX1wpCmhiF","colab_type":"code","colab":{}},"cell_type":"code","source":["tokenizer = Tokenizer(num_words=MAX_NB_WORDS,filters=\"\")\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y2-784r2mhiP","colab_type":"code","colab":{}},"cell_type":"code","source":["embeddings_index = {}\n","f = open('/content/gdrive/My Drive/NN/glove.6B.50d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t2BnBYCemhiV","colab_type":"code","colab":{}},"cell_type":"code","source":["data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","# data = np.array(sequences)\n","labels_1 = to_categorical(np.asarray(labels),num_classes=2)\n","\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels_1.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cPDD2rwpmhih","colab_type":"text"},"cell_type":"markdown","source":["index_word = dict(zip(word_index.values(),word_index.keys()))"]},{"metadata":{"id":"1AVSxrjxmhij","colab_type":"text"},"cell_type":"markdown","source":["new_data = []\n","for data_row in data[:1000]:\n","    new_row = []\n","    for word_idx in data_row:\n","        new_word = embeddings_index.get(index_word[word_idx])\n","#         print(type(new_word))\n","        if  type(new_word).__module__== 'numpy':\n","            new_row.append(new_word)\n","        else:\n","            new_row.append([0]*100)\n","    new_data.append(np.array(new_row))\n","# new_data = np.array(new_data)"]},{"metadata":{"id":"a9638glSmhik","colab_type":"text"},"cell_type":"markdown","source":["new_data_2 = list(map( lambda y: np.array(list(map(lambda x: list(x), y))), new_data))"]},{"metadata":{"id":"C4-PVXSgmhim","colab_type":"text"},"cell_type":"markdown","source":["new_data_2 = np.array(new_data_2,dtype=object)"]},{"metadata":{"id":"vTzyF31_mhin","colab_type":"code","colab":{}},"cell_type":"code","source":["EMBEDDING_DIM=50\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E4sKqTHnmhiy","colab_type":"code","colab":{}},"cell_type":"code","source":["# split the data into a training set and a validation set\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels_1 = labels_1[indices]\n","nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n","\n","x_train = data[:-nb_validation_samples]\n","y_train = labels_1[:-nb_validation_samples]\n","x_val = data[-nb_validation_samples:-int(nb_validation_samples/2)]\n","y_val = labels_1[-nb_validation_samples:-int(nb_validation_samples/2)]\n","x_test = data[-int(nb_validation_samples/2):]\n","y_test = labels_1[-int(nb_validation_samples/2):]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uNvMJwJhmhi3","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Embedding,Input, Conv1D, MaxPooling1D, Dense, Flatten, Reshape, Dropout, LSTM, Activation\n","from keras.models import Model, Sequential\n","\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True)\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","embedded_sequences = embedding_layer(sequence_input)\n","model_1 = Model(sequence_input, embedded_sequences)\n","model_1.summary()\n","model = Sequential()\n","model.add(model_1)\n","model.add(Conv1D(64, 5, activation='relu',input_shape=(None, 500)))\n","\n","model.add(Dropout(0.25))\n","model.add(LSTM(64))\n","model.add(Dense(50))\n","model.add(Dense(2,activation=\"softmax\"))\n","\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8KhgUEBWmhi-","colab_type":"text"},"cell_type":"markdown","source":["### Load weights "]},{"metadata":{"id":"0LYYhrTrmhjA","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import model_from_json\n","json_file = open('/content/gdrive/My Drive/NN/model_tweets_new.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(loaded_model_json)\n","# load weights into new model`\n","model.load_weights(\"model_tweets_new.h5\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z5MYbCpDmhjQ","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"KGVZs_mkmhja","colab_type":"code","colab":{}},"cell_type":"code","source":["# fitting the data\n","model.fit(x_train, y_train, validation_data=(x_val, y_val),\n","          epochs=6, batch_size=1280)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M7d68TI9mhjj","colab_type":"code","colab":{}},"cell_type":"code","source":["score = model.evaluate(x_test,y_test, batch_size=1280)\n","print(\"Loss: \"+str(score[0]))\n","print(\"Accuracy: \"+str(score[1]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NqWQQTS9mhjn","colab_type":"text"},"cell_type":"markdown","source":["### Check Custom Sentences"]},{"metadata":{"id":"-BMnEF9Umhjo","colab_type":"raw"},"cell_type":"markdown","source":["x_test_1 = [\"this movie is not so good\",\"I dont like it\", \"this is neutral\"]"]},{"metadata":{"id":"PHHKzBifmhjp","colab_type":"raw"},"cell_type":"markdown","source":["sequences = tokenizer.texts_to_sequences(x_test_1)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', data.shape)\n","score = model.evaluate(data, y_test)\n","\n","print (score)"]},{"metadata":{"id":"qwh0Tzw3mhjq","colab_type":"raw"},"cell_type":"markdown","source":["sequences = tokenizer.texts_to_sequences(\"This is a great movie\")\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', data.shape)"]},{"metadata":{"id":"OOfonR9Dmhj2","colab_type":"text"},"cell_type":"markdown","source":["### Save model"]},{"metadata":{"id":"oZ-He6IGmhj4","colab_type":"code","colab":{}},"cell_type":"code","source":["model_json = model.to_json()\n","with open(\"model_tweets_new.json\", \"w\") as json_file:\n","    json_file.write(model_json)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"whRHWs97mhj8","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save_weights(\"model_tweets_new.h5\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1nKfD_5-mhkA","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}